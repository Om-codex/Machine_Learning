# ğŸ§  Machine Learning Repository

Welcome to my **Machine Learning Learning Journey!** ğŸš€  

This repository documents my progress â€” from the mathematical foundations of ML algorithms to full-fledged implementations. It includes from-scratch derivations, Scikit-learn applications, and clear visualizations for better understanding.

---

## ğŸ“˜ Repository Structure

### ğŸ”¹ Core Chapters
- **Chapter1**, **Chapter2**, **Chapter3** â€” Foundational concepts, theory, and introductory experiments.

---

## ğŸ“Š Regression Algorithms

* **Linear Regression**
  * Implemented using Normal Equation, Scikit-learn, and custom Gradient Descent.
* **Multiple Linear Regression**
  * Extension of Linear Regression with multiple features.
* **Polynomial Regression**
  * Demonstrating underfitting vs. overfitting.
* **Ridge Regression**, **Lasso**, **Elastic Net**
  * Regularization techniques to handle multicollinearity and prevent overfitting.

---

## ğŸ§® Optimization Techniques

* **Gradient Descent**
  * Batch, Stochastic, and Mini-Batch implementations.
  * Detailed analysis of convergence and learning rates.

---

## ğŸ¯ Classification Algorithms

* **Logistic Regression**
  * Binary and multiclass classification.
* **Naive Bayes**
  * Probabilistic approach for categorical and text data.
* **Support Vector Machines (SVMs)**
  * Linear and RBF kernel-based models.
* **Decision Trees**
  * Visualization and interpretability.
* **PCA (Principal Component Analysis)**
  * Dimensionality reduction and visualization of feature spaces.

---

## ğŸŒ² Ensemble Learning

* **Bagging Ensemble**
  * Combining weak learners to reduce variance.
* **Voting Ensemble**
  * Hard and soft voting classifiers.
* **Gradient Boosting**
  * Step-by-step implementation on the Iris dataset (`gradient-boosting-classifier-on-iris.ipynb`).
* *(Upcoming)* AdaBoost, XGBoost, and Random Forests.

---

## ğŸ’¡ Key Concepts Covered

* **Data Preprocessing:** Scaling, encoding, handling missing values.  
* **Model Evaluation:** RÂ², Accuracy, Precision, Recall, F1-score, ROC-AUC.  
* **Biasâ€“Variance Tradeoff:** Theory and practical illustrations.  
* **Regularization & Optimization:** Ridge, Lasso, Elastic Net, Gradient Descent.  
* **Dimensionality Reduction & Feature Selection.**

---

## ğŸ› ï¸ Technologies Used

* **Language:** Python ğŸ  
* **Libraries:**  
  `NumPy`, `Pandas`, `Matplotlib`, `Seaborn`, `Scikit-learn`, `mlxtend`, and `TensorFlow (for datasets)`

---

## ğŸ“š Learning Focus

Each folder contains:
- Jupyter notebooks explaining the math intuition ğŸ‘¨â€ğŸ«  
- Implementation from scratch and via Scikit-learn âš™ï¸  
- Visualizations and model evaluations ğŸ“ˆ  

---

â­ **If you find this repository helpful, feel free to star it!**  
Every star helps support continued learning and sharing ğŸ˜Š
